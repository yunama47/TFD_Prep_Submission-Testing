{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunama47/TFD_Prep_Submission-Testing/blob/main/TFD_Prep_Submission_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HkApejvi3F8"
      },
      "source": [
        "# Preparing Environtment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WSs0r3NyyRok"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py38\" --user\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SCzSruiyV41"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT!!\n",
        "# Before you continue, reload the web page first!\n",
        "# Then go to Runtime > Change runtime type > choose py38"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYEDC_L7yrUI"
      },
      "outputs": [],
      "source": [
        "# If this shows python version 3.8, then you are good to go.\n",
        "import sys\n",
        "print(\"User Current Version:-\", sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZrHqa1OyyrQ"
      },
      "outputs": [],
      "source": [
        "# Install protobuf version 3.20.3\n",
        "# Restart the runtime if prompted\n",
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVzU10y6j234"
      },
      "source": [
        "install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJNbg9RJy7BP"
      },
      "outputs": [],
      "source": [
        "# Restart the runtime if prompted\n",
        "!printf \"tensorflow==2.9.0\\ntensorflow-datasets==4.6.0\\nPillow==9.1.1\\npandas==1.4.2\\nnumpy==1.22.4\\nscipy==1.7.3\" >> requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPnPWd-z9n0"
      },
      "outputs": [],
      "source": [
        "# test the environment\n",
        "def test_env():\n",
        "    import sys\n",
        "    if sys.base_prefix == sys.prefix:\n",
        "        print(\"You are probably not using virtual environment on this project.\")\n",
        "    assert sys.version_info[:2] == (3, 8), \"Use python 3.8\"\n",
        "    print(\"You are currently using Python\", sys.version)\n",
        "    print('version & environment are all good')\n",
        "\n",
        "def test_packages():\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_datasets as tfds\n",
        "    import PIL\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import scipy\n",
        "    assert tf.__version__ == '2.9.0', f\"Tensorflow version isn't 2.9.0, yours currently in {tf.__version__}\"\n",
        "    assert tfds.__version__ == '4.6.0', f\"Tensorflow dataset version isn't 4.6.0, yours currently in {tfds.__version__}\"\n",
        "    assert PIL.__version__ == '9.1.1', f\"Pillow version isn't 9.1.1, yours currently in {PIL.__version__}\"\n",
        "    assert pd.__version__ == '1.4.2', f\"pandas version isn't 1.4.2, yours currently in {pd.__version__}\"\n",
        "    assert np.__version__ == '1.22.4', f\"numpy version isn't 1.22.4, yours currently in {np.__version__}\"\n",
        "    assert scipy.__version__ == '1.7.3', f\"scipy version isn't 1.7.3, yours currently in {scipy.__version__}\"\n",
        "    print('Packages are all good')\n",
        "\n",
        "test_env()\n",
        "test_packages()\n",
        "print('All good. You are ready to go.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpthXJqK1Bbs"
      },
      "source": [
        "# Code starts here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgplN0FR07gt"
      },
      "outputs": [],
      "source": [
        "# Notebook originally made by Adval\n",
        "print('Its ya boy Adval @justadval')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuOuTSkzkhHj"
      },
      "source": [
        "## TFD Prep Submission Testing\n",
        "\n",
        "test cases by Wahyu  ,feel free to ask me if you have any question\n",
        "\n",
        "discord : yunama (ML-05 | Wahyu Purnama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U_xz6-5G1mn0"
      },
      "outputs": [],
      "source": [
        "# download dataset_loader module\n",
        "!wget https://github.com/yunama47/TFD_Prep_Submission-Testing/raw/main/datasets_loader.pyc -o dataset_loader.pyc\n",
        "# test importing module\n",
        "import datasets_loader as dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N_CrntIelpPL"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Download your Submission zip file (optional)\n",
        "import zipfile\n",
        "import urllib.request\n",
        "#@markdown \n",
        "#@markdown your zip file's id in google drive\n",
        "file_id = '' #@param {type:\"string\"}\n",
        "url = f'https://drive.google.com/u/2/uc?id={file_id}&export=download&confirm=t'\n",
        "local_zip = 'Submission X.zip'\n",
        "urllib.request.urlretrieve(url,local_zip)\n",
        "with zipfile.ZipFile(local_zip, 'r') as zip:\n",
        "  zip.extractall('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg1aUiG_kuD_"
      },
      "source": [
        "### test cases Submission A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRFrGBlekyNF"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import datasets_loader as dl\n",
        "from tensorflow import keras\n",
        "\n",
        "# change this to your models directory, move all your model h5 there\n",
        "MODEL_DIR = 'Submission_A'  # make sure this directory contain all the h5 models\n",
        "assert os.path.isdir(MODEL_DIR), \"Model directory doesn't exist\"\n",
        "DATA_DIR = 'data'\n",
        "\n",
        "# Loading all datasets\n",
        "X, Y = dl.loadLinRegDataset('A')\n",
        "train_generator, validation_generator = dl.loadHorseHuman(localDataDir=DATA_DIR, download=True)\n",
        "training_padded, training_labels, test_padded, test_labels = dl.loadIMDB(data_dir=DATA_DIR)\n",
        "train_set, test_set = dl.loadSunspots(local_csv = 'data/sunspots.csv',download = True)\n",
        "\n",
        "\n",
        "def testA1():\n",
        "    global X, Y\n",
        "    model_A1 = keras.models.load_model(os.path.join(MODEL_DIR,'model_A1.h5'))\n",
        "    try:\n",
        "        result = model_A1.evaluate(X, Y, verbose=0)\n",
        "        print(f'\\033[96m result A1 : loss = {result}')\n",
        "        assert (result < 1e-4), 'accuracy A1 didnt pass !'\n",
        "    except TypeError:\n",
        "        result = model_A1.evaluate(X, Y, verbose=0)[0]\n",
        "        print(f'\\033[96m result A1 : loss = {result}')\n",
        "        assert (result < 1e-4), 'accuracy A1 didnt pass !'\n",
        "\n",
        "def testA2():\n",
        "    global train_generator, validation_generator\n",
        "    model_A2 = keras.models.load_model(os.path.join(MODEL_DIR,'model_A2.h5'))\n",
        "    _, result_train = model_A2.evaluate(train_generator, verbose=0)\n",
        "    _, result_test = model_A2.evaluate(validation_generator, verbose=0)\n",
        "    print(f'\\033[96m result A2 : val_accuracy = {result_test}, train_accuracy = {result_train}')\n",
        "    assert (result_test > 0.83 and result_train > 0.83), 'accuracy A2 didnt pass !'\n",
        "\n",
        "def testA3():\n",
        "    global train_generator, validation_generator\n",
        "    model_A3 = keras.models.load_model(os.path.join(MODEL_DIR,'model_A3.h5'))\n",
        "    _, result_train = model_A3.evaluate(train_generator, verbose=0)\n",
        "    _, result_test = model_A3.evaluate(validation_generator, verbose=0)\n",
        "    print(f'\\033[96m result A3 : val_accuracy = {result_test}, train_accuracy = {result_train}')\n",
        "    assert (result_test > 0.97 and result_train > 0.97), 'accuracy A3 didnt pass !'\n",
        "\n",
        "def testA4():\n",
        "    global training_padded, training_labels, test_padded, test_labels\n",
        "    model_A4 = keras.models.load_model(os.path.join(MODEL_DIR,'model_A4.h5'))\n",
        "    _, result_train = model_A4.evaluate(training_padded, training_labels, verbose=0)\n",
        "    _, result_test = model_A4.evaluate(test_padded, test_labels, verbose=0)\n",
        "    print(f'\\033[96m result A4 : val_accuracy = {result_test}, train_accuracy ={result_train}')\n",
        "    assert (result_test > 0.83 and result_train > 0.83), 'accuracy A4 didnt pass !'\n",
        "\n",
        "def testA5():\n",
        "    global train_set, test_set\n",
        "    model_A5 = keras.models.load_model(os.path.join(MODEL_DIR,'model_A5.h5'))\n",
        "    _, result_train = model_A5.evaluate(train_set,  verbose=0)\n",
        "    _, result_test = model_A5.evaluate(test_set,  verbose=0)\n",
        "    print(f'\\033[96m result A5 : val_mae = {result_test}, train_mae = {result_train}')\n",
        "    assert (result_test < 0.15 and result_train < 0.15), 'mae A5 didnt pass !'\n",
        "\n",
        "# comment one if you dont want to test all\n",
        "testA1()\n",
        "testA2()\n",
        "testA3()\n",
        "testA4()\n",
        "testA5()\n",
        "print('\\033[92m All test passed !')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy3ibXf_k2Ac"
      },
      "source": [
        "### test cases Submission B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8bJBbJ_k40f"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import datasets_loader as dl\n",
        "from tensorflow import keras\n",
        "\n",
        "# change this to your models directory, move all your model h5 there\n",
        "MODEL_DIR = 'Submission B'  # make sure this directory contain all the h5 models\n",
        "\n",
        "# Loading all datasets\n",
        "X, Y = dl.loadLinRegDataset('B')\n",
        "(train_images_b2, train_labels_b2), (test_images_b2, testing_labels_b2) = dl.loadFashionMNIST()\n",
        "train_generator, validation_generator = dl.loadRPS(localDataDir='data', download=True)\n",
        "training_padded, training_labels, test_padded, test_labels = dl.loadBBC()\n",
        "train_set, test_set = dl.loadDailyMaxTemp(local_csv='data/daily-min-temperatures.csv', download=True)\n",
        "\n",
        "\n",
        "def testB1():\n",
        "    global X, Y\n",
        "    model_B1 = keras.models.load_model(os.path.join(MODEL_DIR,'model_B1.h5'))\n",
        "    try:\n",
        "        result = model_B1.evaluate(X, Y, verbose=0)\n",
        "        print(f'\\033[96m result B1 : loss = {result}')\n",
        "        assert (result < 1e-3), 'accuracy B1 didnt pass !'\n",
        "    except TypeError:\n",
        "        result = model_B1.evaluate(X, Y, verbose=0)[0]\n",
        "        print(f'\\033[96m result B1 : loss = {result}')\n",
        "        assert (result < 1e-3), 'accuracy B1 didnt pass !'\n",
        "\n",
        "def testB2():\n",
        "    global train_images_b2, train_labels_b2, test_images_b2, testing_labels_b2\n",
        "    model_B2 = keras.models.load_model(os.path.join(MODEL_DIR,'model_B2.h5'))\n",
        "    _, result_train = model_B2.evaluate(train_images_b2, train_labels_b2, verbose=0)\n",
        "    _, result_test = model_B2.evaluate(test_images_b2, testing_labels_b2, verbose=0)\n",
        "    print(f'\\033[96m result B2 : val_accuracy = {result_test}, train_accuracy={result_train}')\n",
        "    assert (result_test > 0.83 and result_train > 0.83), 'accuracy B2 didnt pass !'\n",
        "\n",
        "def testB3():\n",
        "    global train_generator, validation_generator\n",
        "    model_B3 = keras.models.load_model(os.path.join(MODEL_DIR,'model_B3.h5'))\n",
        "    _, result_train = model_B3.evaluate(train_generator, verbose=0)\n",
        "    _, result_test = model_B3.evaluate(validation_generator, verbose=0)\n",
        "    print(f'\\033[96m result B3 : val_accuracy = {result_test}, train_accuracy={result_train}')\n",
        "    assert (result_test > 0.83 and result_train > 0.83), 'accuracy B3 didnt pass !'\n",
        "\n",
        "def testB4():\n",
        "    global training_padded, training_labels, test_padded, test_labels\n",
        "    model_B4 = keras.models.load_model(os.path.join(MODEL_DIR,'model_B4.h5'))\n",
        "    _, result_train = model_B4.evaluate(training_padded, training_labels, verbose=0)\n",
        "    _, result_test = model_B4.evaluate(test_padded, test_labels, verbose=0)\n",
        "    print(f'\\033[96m result B4 : val_accuracy = {result_test}, train_accuracy={result_train}')\n",
        "    assert (result_test > 0.91 and result_train > 0.91), 'accuracy B4 didnt pass !'\n",
        "\n",
        "def testB5():\n",
        "    global train_set,test_set\n",
        "    model_B5 = keras.models.load_model(os.path.join(MODEL_DIR,'model_B5.h5'))\n",
        "    _, result_train = model_B5.evaluate(train_set, verbose=0)\n",
        "    _, result_test = model_B5.evaluate(train_set, verbose=0)\n",
        "    print(f'\\033[96m result B5 : val_mae = {result_test}, train_mae = {result_train}')\n",
        "    assert (result_test < 0.2 and result_train < 0.2), 'mae B5 didnt pass !'\n",
        "\n",
        "# comment one if you dont want to test all\n",
        "testB1()\n",
        "testB2()\n",
        "testB3()\n",
        "testB4()\n",
        "testB5()\n",
        "print('\\033[92m All test passed !')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ltgAuONk58f"
      },
      "source": [
        "### test cases Submission C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZauagqOUk8E1"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import datasets_loader as dl\n",
        "from tensorflow import keras\n",
        "\n",
        "# change this to your models directory, move all your model h5 there\n",
        "MODEL_DIR = 'Submission C'  # make sure this directory contain all the h5 models\n",
        "\n",
        "\n",
        "# Loading all datasets\n",
        "X, Y = dl.loadLinRegDataset('C')\n",
        "(train_images_c2, train_labels_c2), (test_images_c2, testing_labels_c2) = dl.loadMNIST()\n",
        "train_generator, validation_generator = dl.loadCatsAndDogs(localDataDir='data', download=True)\n",
        "training_padded, training_labels, test_padded, test_labels = dl.loadSarcasm(localJson='data/sarcasm.json', download=True)\n",
        "train_set, test_set = dl.loadDailyMinTemp(local_csv='data/daily-min-temperatures.csv', download=True)\n",
        "\n",
        "\n",
        "def testC1():\n",
        "    global X, Y\n",
        "    model_C1 = keras.models.load_model(os.path.join(MODEL_DIR,'model_C1.h5'))\n",
        "    try:\n",
        "        result = model_C1.evaluate(X, Y, verbose=0)\n",
        "        print(f'\\033[96m result C1 : loss = {result}')\n",
        "        assert (result < 1e-4), 'accuracy C1 didnt pass !'\n",
        "    except TypeError:\n",
        "        result = model_C1.evaluate(X, Y, verbose=0)[0]\n",
        "        print(f'\\033[96m result C1 : loss = {result}')\n",
        "        assert (result < 1e-4), 'accuracy C1 didnt pass !'\n",
        "\n",
        "def testC2():\n",
        "    global train_images_c2, train_labels_c2, test_images_c2, testing_labels_c2\n",
        "    model_C2 = keras.models.load_model(os.path.join(MODEL_DIR,'model_C2.h5'))\n",
        "    _, result_train = model_C2.evaluate(train_images_c2, train_labels_c2, verbose=0)\n",
        "    _, result_test = model_C2.evaluate(test_images_c2, testing_labels_c2, verbose=0)\n",
        "    print(f'\\033[96m result C2 : val_accuracy = {result_test}, train_accuracy={result_train}')\n",
        "    assert (result_test > 0.91 and result_train > 0.91), 'accuracy C2 didnt pass !'\n",
        "\n",
        "def testC3(): # cats dogs\n",
        "    global train_generator, validation_generator\n",
        "    model_C3 = keras.models.load_model(os.path.join(MODEL_DIR,'model_C3.h5'))\n",
        "    _, result_train = model_C3.evaluate(train_generator, verbose=0)\n",
        "    _, result_test = model_C3.evaluate(validation_generator, verbose=0)\n",
        "    print(f'\\033[96m result C3 : val_accuracy = {result_test}, train_accuracy={result_train}')\n",
        "    assert (result_test > 0.72 and result_train > 0.72), 'accuracy C3 didnt pass !'\n",
        "\n",
        "def testC4():\n",
        "    global training_padded, training_labels, test_padded, test_labels\n",
        "    model_C4 = keras.models.load_model(os.path.join(MODEL_DIR,'model_C4.h5'))\n",
        "    _, result_train = model_C4.evaluate(training_padded, training_labels, verbose=0)\n",
        "    _, result_test = model_C4.evaluate(test_padded, test_labels, verbose=0)\n",
        "    print(f'\\033[96m result C4 : val_accuracy = {result_test}, train_accuracy={result_train}')\n",
        "    assert (result_test > 0.75 and result_train > 0.75), 'accuracy C4 didnt pass !'\n",
        "\n",
        "def testC5():\n",
        "    global train_set,test_set\n",
        "    model_C5 = keras.models.load_model(os.path.join(MODEL_DIR,'model_C5.h5'))\n",
        "    _, result_train = model_C5.evaluate(train_set, verbose=0)\n",
        "    _, result_test = model_C5.evaluate(train_set, verbose=0)\n",
        "    print(f'\\033[96m result C5 : val_mae = {result_test}, train_mae = {result_train}')\n",
        "    assert (result_test < 0.19 and result_train < 0.19), 'mae C5 didnt pass !'\n",
        "\n",
        "# comment one if you dont want to test all\n",
        "testC1()\n",
        "testC2()\n",
        "testC3()\n",
        "testC4()\n",
        "testC5()\n",
        "print('\\033[92m all test passed!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py38",
      "name": "py38"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
